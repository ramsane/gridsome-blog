<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://ramsane.github.io/</id>
    <title>ramsane</title>
    <updated>2020-05-30T16:44:00.048Z</updated>
    <generator>Gridsome Feed Plugin</generator>
    <link rel="alternate" href="https://ramsane.github.io"/>
    <link rel="self" href="https://ramsane.github.io/feed.atom"/>
    <subtitle>RamSane Blog</subtitle>
    <entry>
        <title type="html"><![CDATA[Cross Entropy : An intuitive explanation with Entropy and  KL-Divergence]]></title>
        <id>https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence/</id>
        <link href="https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence/"/>
        <updated>2020-06-01T00:00:00.000Z</updated>
        <content type="html"><![CDATA[Cross-Entropy is something that you see over and over in machine learning and deep learning. This article explains it from Information theory prespective and try to connect the dots. KL-Divergence is also very important and is used in Decision Trees and generative models like Variational Auto Encoders.]]></content>
    </entry>
</feed>