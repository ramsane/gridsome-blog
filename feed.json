{
    "version": "https://jsonfeed.org/version/1",
    "title": "ramsane",
    "home_page_url": "https://ramsane.github.io",
    "feed_url": "https://ramsane.github.io/feed.json",
    "description": "RamSane Blog",
    "items": [
        {
            "id": "https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence/",
            "content_html": "Cross-Entropy is something that you see over and over in machine learning and deep learning. This article explains it from Information theory prespective and try to connect the dots. KL-Divergence is also very important and is used in Decision Trees and generative models like Variational Auto Encoders.",
            "url": "https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence/",
            "title": "Cross Entropy : An intuitive explanation with Entropy and  KL-Divergence",
            "date_modified": "2020-06-01T00:00:00.000Z"
        }
    ]
}