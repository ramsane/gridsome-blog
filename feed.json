{
    "version": "https://jsonfeed.org/version/1",
    "title": "ramsane",
    "home_page_url": "https://ramsane.github.io",
    "feed_url": "https://ramsane.github.io/feed.json",
    "description": "RamSane Blog",
    "items": [
        {
            "id": "https://ramsane.github.io/articles/anatomy-of-suppor-vector-machines-part-1/",
            "content_html": "Support Vector machines are one of the most intriguing concept in whole of machine learning. When I was learning about it, I had so many questions and the content on the internet is either incomplete or incorrect. In this series of articles, I tried to answer all the questions that comes to our mind and those that will help you understand the concept better.",
            "url": "https://ramsane.github.io/articles/anatomy-of-suppor-vector-machines-part-1/",
            "title": "Anatomy of Support Vector Machines : Hard margin and Dual formulation",
            "date_modified": "2021-01-06T00:00:00.000Z"
        },
        {
            "id": "https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence/",
            "content_html": "Cross-Entropy is something that you see over and over in machine learning and deep learning. This article explains it from Information theory prespective and try to connect the dots. KL-Divergence is also very important and is used in Decision Trees and generative models like Variational Auto Encoders.",
            "url": "https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence/",
            "title": "Cross Entropy : An intuitive explanation with Entropy and  KL-Divergence",
            "date_modified": "2020-06-01T00:00:00.000Z"
        }
    ]
}