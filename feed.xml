<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>ramsane</title>
        <link>https://ramsane.github.io</link>
        <description>RamSane Blog</description>
        <lastBuildDate>Sat, 30 May 2020 16:44:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>Gridsome Feed Plugin</generator>
        <atom:link href="https://ramsane.github.io/feed.atom" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Cross Entropy : An intuitive explanation with Entropy and  KL-Divergence]]></title>
            <link>https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence/</link>
            <guid>https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence/</guid>
            <pubDate>Mon, 01 Jun 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Cross-Entropy is something that you see over and over in machine learning and deep learning. This article explains it from Information theory prespective and try to connect the dots. KL-Divergence is also very important and is used in Decision Trees and generative models like Variational Auto Encoders.]]></content:encoded>
        </item>
    </channel>
</rss>